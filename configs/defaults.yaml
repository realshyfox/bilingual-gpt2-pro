# Default Configuration for Bilingual GPT-2 Training
# This file contains sensible defaults for all parameters

dataset:
  train_path: null  # Required: path to training data
  val_path: null    # Optional: path to validation data
  analysis:
    # This section is populated by the dataset analyzer
    total_tokens: null
    dataset_type: null
    languages: []

task:
  type: pre-training  # Options: pre-training, fine-tuning

tokenizer:
  type: unigram  # Options: bpe, unigram, wordpiece
  vocab_size: 50257  # Will be overridden by analyzer recommendation
  character_coverage: 0.9995  # For SentencePiece
  special_tokens:
    - "<pad>"
    - "<unk>"
    - "<s>"
    - "</s>"

model:
  type: gpt2
  size_preset: mini  # Options: tiny, mini, small, medium
  
  # Architecture (from preset)
  embed_dim: 768
  num_layers: 12
  num_heads: 12
  max_seq_len: 1024
  vocab_size: 50257
  
  # Regularization
  dropout: 0.1
  attention_dropout: 0.1
  
  # Modern features
  use_flash_attention: true
  position_encoding: rope  # Options: rope, alibi, sliding, learned
  activation: gelu  # Options: gelu, swiglu, relu
  normalization: layernorm  # Options: layernorm, rmsnorm
  
  # Advanced (optional)
  use_gqa: false  # Grouped Query Attention
  gqa_num_groups: null

training:
  # Training duration
  max_steps: 500000
  max_epochs: null  # Alternative to max_steps
  
  # Batch configuration
  batch_size: 16
  gradient_accumulation_steps: 4
  
  # Optimization
  optimizer: adamw
  learning_rate: 3.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-8
  
  # Learning rate schedule
  scheduler: cosine  # Options: cosine, linear, constant
  warmup_steps: 2000
  min_lr_ratio: 0.1
  
  # Precision and stability
  precision: bf16  # Options: fp32, fp16, bf16
  gradient_clipping: 1.0
  gradient_checkpointing: false
  
  # Distributed training
  zero_stage: 2  # DeepSpeed ZeRO: 0, 1, 2, or 3
  
  # Logging and checkpointing
  log_interval: 100
  eval_interval: 5000
  save_interval: 10000
  
  # Validation
  eval_steps: 500  # Number of eval steps per evaluation
  
  # Data loading
  num_workers: 4
  prefetch_factor: 2

hardware:
  # Detected by hardware detector
  num_gpus: 1
  vram_per_gpu: 16
  supports_bf16: true
  supports_fp16: true

output:
  dir: outputs/model
  save_total_limit: 3  # Keep only last N checkpoints
  
  logging:
    use_wandb: false
    use_tensorboard: true
    project_name: bilingual-gpt2
    run_name: null  # Auto-generated if null
    log_dir: logs

# Reproducibility
seed: 42
deterministic: false  # Set true for full reproducibility (slower)
